# ETL & Apache Airflow задания
## Задание 1: Основы ETL

**Вопрос:** Опишите, что такое процесс ETL и какие его ключевые этапы. Приведите пример сценария его использования.

**Ответ:** 
ETL (Extract, Transform, Load) - процесс обработки данных, который идет по следующей логике:

2. Извлечение данных из первичной системы. На этом этапе данные извлекаются из различных источников, таких как базы данных, файлы (CSV, JSON, XML), API, облачные хранилища и т.д.
2. Изменение/Обработка данных (Преобразование). Данные очищаются, фильтруются, агрегируются и преобразуются в формат, подходящий для целевой системы. Это может включать удаление дубликатов, приведение данных к единому формату, вычисление новых полей и т.д.
3. Загрузка данных в конечную систему. Преобразованные данные загружаются в целевую систему, такую как хранилище данных, база данных или аналитическая платформа.

Этот процесс используют для различных интеграционных решений, чтобы впоследствии использовать подготовленные и загруженные данные в своей системе или функции.

**Пример использования:**
Предположим, система Х хочет провести комплексный анализ для расчета определенных показателей. Однако необходимые данные хранятся в N различных системах, и в системе Х они отсутствуют. При этом в системах N данные могут быть связаны по общим идентификаторам объекта. В этом случае процесс ETL будет выглядеть следующим образом:

1. Извлечение: Данные извлекаются из N систем.
2. Преобразование: Данные объединяются по общему идентификатору, очищаются и преобразуются. Если требуется, выполняются расчеты или создаются новые поля.
3. Загрузка: Подготовленные данные загружаются в систему Х для дальнейшего анализа.





## Задание 2: Основы Apache Airflow

**Вопрос:** Что такое Apache Airflow и какова его основная цель? Каковы основные компоненты Airflow?

**Ответ:** 
Это платформа планирования и мониторинга рабочих процессов (workflows). Airflow необходим для оркестрации множества шагов и автоматизации пайплайнов обработки данных.
С его помощью есть возможность просматривать аналитическую статистику по отработке запущенных процессов, их состоянии и скорости.
Основная его цель упростить работу для интеграционных взаимодействий связанных с переливкой данных/общением между системами.

Основные компоненты AirFlow:
1. DAG - Кирпич всего пайплайна, внутри которого могут находиться задачи(алгоритм обработки) или etl процесс.
2. Tasks - Инструмент для вызова логических операций, например для системных скриптов/кода питона/sql запроса.
3. Operators - подготовленный инструмент/метод внутри AirFlow, который обозначает например запуск кода питона
4. Scheduler - инструмент для подготовки крон по времени
5. Executor - распределение задач 
6. WEB UI - интерфейс AirFlow для пользователя
7. Workers - процессы или ноды для выполнения задач AirFlow

